"""
ragas_evaluator.py

Evaluates responses generated by a Retrieval-Augmented Generation (RAG) pipeline using RAGAS (Retrieval-Augmented Generation Assessment Scores) metrics. 

Key Features:
- Configures the evaluation LLM (OpenAI) and embedding model (Hugging Face).
- Loads RAG-generated output CSV and converts it into Hugging Face Dataset format.
- Applies standard RAGAS metrics: context precision, context recall, faithfulness, and answer similarity.
- Outputs evaluation results as a structured CSV file for benchmarking.
"""

import os
import logging
import torch

import pandas as pd

from datasets import Dataset

from ragas import evaluate
from ragas.metrics import context_precision, context_recall, faithfulness, answer_similarity

from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings


class RAGASEvaluator:
    def __init__(self) -> None:
        """
        Initialize the RAGAS evaluator with the selected LLM backend and a Hugging Face embedding model.
        """
        self.embedding_model_name = os.getenv("EMBEDDING_MODEL")

        # Initialize the evaluator with:
        # - OpenAI LLM via LangChain for consistent and trusted answer analysis
        # - temperature=0 to ensure deterministic and reproducible outputs
        # - Hugging Face embeddings for semantic similarity scoring between context and answers
        self.llm = ChatOpenAI(
            model=os.getenv("GPT3_TURBO_MODEL"),
            api_key=os.getenv("OPENAI_API_KEY"),
            temperature=0,
            seed=42
        )

        # Load semantic embedding model from Hugging Face
        self.embedding_model = HuggingFaceEmbeddings(
            model_name=self.embedding_model_name,
            model_kwargs={"device": torch.device("cuda:1")}
        )

    def run_evaluation(self, dataset: Dataset) -> pd.DataFrame:
        """
        Run the full suite of RAGAS metrics and return evaluation results as a DataFrame.

        Args:
            dataset (Dataset): Hugging Face Dataset containing the RAG pipeline output.

        Returns:
            pd.DataFrame: DataFrame containing per-record evaluation scores.
        """
        logging.info("â³ Running RAGAS metric evaluation...")

        # Evaluate using multiple quality metrics from the RAGAS library
        ragas_result = evaluate(
            dataset=dataset,
            llm=self.llm,
            embeddings=self.embedding_model,
            metrics=[
                context_precision,   # Measures precision of retrieved context dataset
                context_recall,      # Measures recall of relevant dataset in the retrieved context
                faithfulness,        # Ensures the generated answer doesn't hallucinate beyond context
                answer_similarity    # Compares answer semantic similarity to an expected answer
            ]
        )

        logging.info("âœ… RAGAS evaluation completed.")

        # Convert RAGAS output into a Pandas DataFrame for reporting
        return ragas_result.to_pandas()

    def evaluate_from_file(self, result_path: str, output_eval_path: str) -> None:
        """
        Run RAGAS evaluation on a CSV file and export results.

        Args:
            result_path (str): Path to RAG result CSV file.
            output_eval_path (str): Path to save the evaluation results CSV.
        """
        logging.info(f"ðŸ“¥ Loading RAG results from {result_path}...")

        # 1.Load the RAG output as DataFrame
        df = pd.read_csv(result_path)

        # NOTE: RAGAS expects the 'contexts' column to be a list of strings, not a single string
        # 2. We split the combined resume string back into a list for the evaluator
        df["contexts"] = df["contexts"].apply(lambda row: row.split("=============="))

        # 3. Convert the dataset to Hugging Face Dataset format
        dataset = Dataset.from_pandas(df)

        # 4. Evaluate the dataset using the RAGAS metrics
        result_df = self.run_evaluation(dataset)

        # 5. Save the evaluation output to the target CSV path
        result_df.to_csv(output_eval_path, index=False)

        logging.info(f"âœ… Final evaluation saved to: {output_eval_path}")
